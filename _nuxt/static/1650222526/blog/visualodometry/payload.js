__NUXT_JSONP__("/blog/visualodometry", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT){return {data:[{article:{slug:"visualodometry",description:"Visual Odometry is an incremental process which estimates the 3D pose of the camera from visual data.The final goal of visual odometry is to estimate the movement of the camera in the environment. This research area is relatively new and several methods and algorithm are already published.",title:"RGB-D Visual Odometry summary",date:"2020-10-10T00:00:00.000Z",img:T,lang:"en",createdAt:"2019-03-22T10:58:51.640Z",tocgenerate:U,publish:U,tags:["Programming","Research","Python"],toc:[{id:V,depth:C,text:W},{id:X,depth:C,text:Y},{id:Z,depth:C,text:_},{id:$,depth:C,text:aa},{id:ab,depth:r,text:ac},{id:ad,depth:r,text:ae},{id:af,depth:r,text:ag},{id:ah,depth:r,text:ai},{id:aj,depth:r,text:ak},{id:al,depth:C,text:am},{id:an,depth:r,text:ao},{id:ap,depth:r,text:aq},{id:ar,depth:r,text:as},{id:at,depth:r,text:au},{id:av,depth:r,text:"Semi-dense Monocular VO algorithms Schops et al."},{id:aw,depth:r,text:ax},{id:ay,depth:C,text:az},{id:aA,depth:r,text:aB},{id:aC,depth:r,text:aD},{id:aE,depth:r,text:aF},{id:aG,depth:C,text:aH},{id:aI,depth:r,text:aJ},{id:aK,depth:C,text:aL}],body:{type:"root",children:[{type:b,tag:e,props:{},children:[{type:b,tag:q,props:{},children:[{type:a,value:"Author"}]}]},{type:a,value:c},{type:b,tag:A,props:{},children:[{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"Hamed Jafarzadeh - "},{type:b,tag:f,props:{href:"mailto:Hamed.Jafarzadeh@Skoltech.ru"},children:[{type:a,value:"Hamed.Jafarzadeh@Skoltech.ru"}]}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"Skolkovo Institue of science and technology - "},{type:b,tag:f,props:{href:"http:\u002F\u002Fsites.skoltech.ru\u002Fmobilerobotics\u002F",rel:[m,n,o],target:p},children:[{type:a,value:"Mobile Robotics Lab"}]}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"Under supervision of "},{type:b,tag:f,props:{href:"https:\u002F\u002Ffaculty.skoltech.ru\u002Fpeople\u002Fgonzaloferrer",rel:[m,n,o],target:p},children:[{type:a,value:"Dr. Gonzalo Ferrer"}]}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:b,tag:f,props:{href:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry",rel:[m,n,o],target:p},children:[{type:a,value:"Github repo"}]}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:D,props:{id:V},children:[{type:b,tag:f,props:{href:"#abstract",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:W}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Visual Odometry is an incremental process which estimates the 3D pose of the camera from visual data."},{type:b,tag:q,props:{},children:[{type:a,value:"The final goal of visual odometry is to estimate the movement of the camera in the environment."}]},{type:a,value:" This research area is relatively new and several methods and algorithm are already published. In  [the reference article][https:\u002F\u002Flink.springer.com\u002Farticle\u002F10.1007\u002Fs11554-017-0670-y] , they compared several RGB-D Visual Odometry techniques to the date (2017).They used a mobile device equipped with a RGB-D camera and they measure the accuracy of each algorithm as well as CPU load. In this summary, I will summarize the main points of the article to outline the main ideas and achievements, additionally I will add some other references that helped me to understand the points better."}]},{type:a,value:c},{type:b,tag:D,props:{id:X},children:[{type:b,tag:f,props:{href:"#introduction",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:Y}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Depth sensors like kinect are being used in different application and industries in order to provide 3D data at relatively low cost. "},{type:b,tag:f,props:{href:"(https:\u002F\u002Fpdfs.semanticscholar.org\u002F63e1\u002Fdffc19c3b4e99ae22ec60d10eaaafd608bcb.pdf)"},children:[{type:a,value:"Khoshelham et al"}]},{type:a,value:" evaluated experimentally the  accuracy of kinect sensors and proposed a "},{type:b,tag:v,props:{},children:[{type:a,value:"noise model"}]},{type:a,value:" which explains why the depth error grows quadratically with the distance to the objects. The accuracy also depends on the tilt of the surface normal w.r.t the camera viewpoint and the properties of the object material. There were several attempts and products which you can find in "},{type:b,tag:f,props:{href:"#Small-size-and-embedded-depth-sensors"},children:[{type:a,value:"small size and embedded depth sensor section"}]},{type:a,value:" you can find some informations in this regard. "},{type:b,tag:s,props:{alt:"Fig 1",src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002FSelection_055.png"},children:[]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Reference paper authors proposed a benchmark and an evaluation of state-of-the-art Visual Odomtery (VO) algorithms suitable for running in real time on mobile devices with RGB-D sensor. Aim of this summary is on different algorithms and their performance, for more information on Visual Odometry and more detail information, reader can check [the reference article][https:\u002F\u002Flink.springer.com\u002Farticle\u002F10.1007\u002Fs11554-017-0670-y] ."}]},{type:a,value:c},{type:b,tag:D,props:{id:Z},children:[{type:b,tag:f,props:{href:"#keynotes-from-the-reference-articlehttpslinkspringercomarticle101007s11554-017-0670-y",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:_}]},{type:a,value:c},{type:b,tag:A,props:{},children:[{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"They used a small-baseline RGB-D camera (PrimeSense)"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Some papers and methods used filters on depth sensors to remove noises and then performing VO"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"There are several techniques for mitigating the noise in depth sensors"}]},{type:a,value:c},{type:b,tag:A,props:{},children:[{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"frame-to-frame"}]},{type:a,value:" matching strategy : comparing each frame with its previous frame "},{type:b,tag:u,props:{},children:[{type:a,value:"however"}]},{type:a,value:" it leads to a large drift of the estimated trajectory as the pose errors are cumulated "},{type:b,tag:u,props:{},children:[{type:a,value:"so"}]},{type:a,value:" some methods are using"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"frame-to-keyframe"}]},{type:a,value:" matching strategy : choosing a high quality frame as the key-frame and matching subsequent frames with this frame, until the next key frame is chosen"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Some methods are using IMU in order to improve the estimations quality"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"frame-to-model"}]},{type:a,value:" are using for building a model of the explored scene and using this model to align the new frames, the model can be a 3D point cloud. This model has the ability of re-localize the device after tracking failure."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Some of the methods are using some post-processing local optimizations to improve the camera pose detection and reduce the trajectory drift. however this method is not applicable to "},{type:b,tag:u,props:{},children:[{type:a,value:"frame-to-to-model"}]},{type:a,value:" estimations"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"To summarize the used methods, we can take a look at this graph, clearly shows different RGB-D Visual Odometry methods"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:D,props:{id:$},children:[{type:b,tag:f,props:{href:"#rgb-d-visual-odometry-methods",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:aa}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:s,props:{alt:"1554888824498",src:T},children:[]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:q,props:{},children:[{type:a,value:"Image-based"}]},{type:a,value:" methods rely on the information of the RGB image and it can be divided in to feature-based methods and direct methods."}]},{type:a,value:c},{type:b,tag:A,props:{},children:[{type:a,value:c},{type:b,tag:l,props:{},children:[{type:b,tag:q,props:{},children:[{type:b,tag:v,props:{},children:[{type:a,value:"Visual features"}]}]},{type:a,value:" | sparse"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:G,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Visual features methods are using local image features to register the current frame to a previous (key)frame. SIFT and SURF features are commonly used for this approach, however their computational costs are very high. BRISK, BRIEF and ORB methods are used instead of SIFT and Surf because of their low computation costs. These methods are perfoming well in highly textured scenes and they tend to fail in poor light conditions and also blurry images."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,L]},children:[{type:b,tag:v,props:{},children:[{type:b,tag:d,props:{className:[g,M,w]},children:[{type:b,tag:d,props:{className:[g,x,w]},children:[{type:a,value:N}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:O}]}]},{type:a,value:c},{type:b,tag:d,props:{className:[g,P,z]},children:[{type:b,tag:d,props:{className:[g,x,z]},children:[{type:a,value:Q}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:R}]}]}]}]}]},{type:a,value:c},{type:b,tag:A,props:{},children:[{type:a,value:c},{type:b,tag:l,props:{},children:[{type:b,tag:q,props:{},children:[{type:b,tag:v,props:{},children:[{type:a,value:"Direct"}]}]},{type:a,value:" | dense"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:G,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"The direct methods are dense method, as the registration uses all the pixels of the images. Under the assumption that the luminosity of the pixels is invariant to small viewpoint changes, they estimate the camera motion that maximizes a photo-consistency criterion between the two considered RGB-D frames."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,L]},children:[{type:b,tag:v,props:{},children:[{type:b,tag:d,props:{className:[g,M,w]},children:[{type:b,tag:d,props:{className:[g,x,w]},children:[{type:a,value:N}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:O}]}]},{type:a,value:c},{type:b,tag:d,props:{className:[g,P,z]},children:[{type:b,tag:d,props:{className:[g,x,z]},children:[{type:a,value:Q}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:R}]}]}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:q,props:{},children:[{type:a,value:aM}]},{type:a,value:" algorithms rely mostly on the information of the depth images. "},{type:b,tag:q,props:{},children:[{type:a,value:aM}]},{type:a,value:" algorithms can work well in poor light conditions as they rely on the 3D data, but on the other hand they might fail with scenes having low structure (e.g. only few planar surfaces)"}]},{type:a,value:c},{type:b,tag:A,props:{},children:[{type:a,value:c},{type:b,tag:l,props:{},children:[{type:b,tag:q,props:{},children:[{type:b,tag:v,props:{},children:[{type:a,value:"3D feature-based"}]}]},{type:a,value:"  | "},{type:b,tag:u,props:{},children:[{type:a,value:"sparse"}]}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:G,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"3D feature-based methods rely on the extraction of salient features on the 3D point clouds. The rigid body transform can be computed by matching the descriptors associated to the features extracted in two frames."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,L]},children:[{type:b,tag:v,props:{},children:[{type:b,tag:d,props:{className:[g,M,w]},children:[{type:b,tag:d,props:{className:[g,x,w]},children:[{type:a,value:N}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:O}]}]},{type:a,value:c},{type:b,tag:d,props:{className:[g,P,z]},children:[{type:b,tag:d,props:{className:[g,x,z]},children:[{type:a,value:Q}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:R}]}]}]}]}]},{type:a,value:c},{type:b,tag:A,props:{},children:[{type:a,value:c},{type:b,tag:l,props:{},children:[{type:b,tag:q,props:{},children:[{type:b,tag:v,props:{},children:[{type:a,value:"ICP - Iterative Closest Point "}]}]},{type:a,value:" | Dense"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:G,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"The Iterative Closest Point methods refer to a class of registration algorithms which try to iteratively minimize the distance between two point clouds without knowing the point correspondences.  The alignment error is computed with a given error metric such as point-to-point or point-to-plane distance, and the process is repeated until this error converges or the maximal number of iterations is reached."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,L]},children:[{type:b,tag:v,props:{},children:[{type:b,tag:d,props:{className:[g,M,w]},children:[{type:b,tag:d,props:{className:[g,x,w]},children:[{type:a,value:N}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:O}]}]},{type:a,value:c},{type:b,tag:d,props:{className:[g,P,z]},children:[{type:b,tag:d,props:{className:[g,x,z]},children:[{type:a,value:Q}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:R}]}]}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:q,props:{},children:[{type:a,value:"Hybrid"}]},{type:a,value:" algorithms try to combine the best of the two worlds in order to handle scenes having either low structure or little texture."}]},{type:a,value:c},{type:b,tag:A,props:{},children:[{type:a,value:c},{type:b,tag:l,props:{},children:[{type:b,tag:q,props:{},children:[{type:b,tag:v,props:{},children:[{type:a,value:"Joint-optimizations"}]}]}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:G,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"The joint-optimization strategy consists in designing an optimization problem which combines equations from depth-based and image-based approaches."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,L]},children:[{type:b,tag:v,props:{},children:[{type:b,tag:d,props:{className:[g,M,w]},children:[{type:b,tag:d,props:{className:[g,x,w]},children:[{type:a,value:N}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:O}]}]},{type:a,value:c},{type:b,tag:d,props:{className:[g,P,z]},children:[{type:b,tag:d,props:{className:[g,x,z]},children:[{type:a,value:Q}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:R}]}]}]}]}]},{type:a,value:c},{type:b,tag:A,props:{},children:[{type:a,value:c},{type:b,tag:l,props:{},children:[{type:b,tag:q,props:{},children:[{type:b,tag:v,props:{},children:[{type:a,value:"Two-stage"}]}]}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:G,props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"They usually use one approach (usually a sparse method) to compute an initial guess of the registration, and use a second approach (usually a dense method) to refine the transformation or just compute it in case of failure of the first approach."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,L]},children:[{type:b,tag:v,props:{},children:[{type:b,tag:d,props:{className:[g,M,w]},children:[{type:b,tag:d,props:{className:[g,x,w]},children:[{type:a,value:N}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:O}]}]},{type:a,value:c},{type:b,tag:d,props:{className:[g,P,z]},children:[{type:b,tag:d,props:{className:[g,x,z]},children:[{type:a,value:Q}]},{type:b,tag:d,props:{className:[g,y]},children:[{type:a,value:R}]}]}]}]}]},{type:a,value:c},{type:b,tag:t,props:{id:ab},children:[{type:b,tag:f,props:{href:"#algortihms",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:ac}]},{type:a,value:c},{type:b,tag:E,props:{},children:[]},{type:a,value:c},{type:b,tag:F,props:{id:"fovis--image-based--feature-based"},children:[{type:b,tag:f,props:{href:"#fovis--image-based--feature-based",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:"Fovis | "},{type:b,tag:u,props:{},children:[{type:a,value:"Image-Based | Feature-Based"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Fovis [20] is a fast visual odometry library developed for micro aerial vehicles (MAV). Feature based with frame-to-key-frame matching strategy."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:s,props:{alt:B,src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002F11554_2017_670_Fig3.gif"},children:[]}]},{type:a,value:c},{type:b,tag:E,props:{},children:[]},{type:a,value:c},{type:b,tag:F,props:{id:"ocv-rgb-d-module"},children:[{type:b,tag:f,props:{href:"#ocv-rgb-d-module",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:"OCV RGB-D Module"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"It is a OpenCV RGB-D module developed by Maria Dimashova and it offers three different type :"}]},{type:a,value:c},{type:b,tag:S,props:{id:"ocv-rgb-d--image-based--direct"},children:[{type:b,tag:f,props:{href:"#ocv-rgb-d--image-based--direct",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:"OCV RGB-D "},{type:b,tag:u,props:{},children:[{type:a,value:aN}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Image-based approach inspired by Steinbrucker et al works  [55] with a frame-to-frame matching strategy. Two hypotheses are made :"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"First, the light intensity of a 3D point is considered constant between frames, then the angular and translational speed are supposed to be constant between two frames. The aglorithm finds the transformation relating two frames by minimizing the difference in intesity between warped current RGB-D frame to the previous one."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:s,props:{alt:B,src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002F11554_2017_670_Fig4_HTML.gif"},children:[]}]},{type:a,value:c},{type:b,tag:S,props:{id:"ocv-icp---depth-based--icp"},children:[{type:b,tag:f,props:{href:"#ocv-icp---depth-based--icp",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:"OCV ICP  "},{type:b,tag:u,props:{},children:[{type:a,value:"| Depth-based | ICP"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"It is inspired by the point cloud registration algorithm of KinectFusion ["},{type:b,tag:f,props:{href:"https:\u002F\u002Flink.springer.com\u002Farticle\u002F10.1007\u002Fs11554-017-0670-y#CR32",rel:[m,n,o],target:p},children:[{type:a,value:"32"}]},{type:a,value:"]. KinectFusion ICP variant is based on a projection based heuristic association function with a point-to-plane error metric. Assuming a small rotation between the two frames, the minimization of the point-to-plane error is reduced to a linear least square problem."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:s,props:{alt:B,src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002F11554_2017_670_Fig5_HTML.gif"},children:[]}]},{type:a,value:c},{type:b,tag:S,props:{id:"ocv-rgbdicp---image-based--direct"},children:[{type:b,tag:f,props:{href:"#ocv-rgbdicp---image-based--direct",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:"OCV RGBDICP  "},{type:b,tag:u,props:{},children:[{type:a,value:aN}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"It is a combination of OCV RGBD and ICP, introduced to solve the linear least square problems of  OCV ICP and RGBD methods."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:s,props:{alt:B,src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002F11554_2017_670_Fig6_HTML.gif"},children:[]}]},{type:a,value:c},{type:b,tag:E,props:{},children:[]},{type:a,value:c},{type:b,tag:F,props:{id:"dvo-dense-visual-odometry"},children:[{type:b,tag:f,props:{href:"#dvo-dense-visual-odometry",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:"DVO (Dense Visual Odometry)"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"It is a direct image-based method with a frame-to-frame matching strategy."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"(I didn't understand it completely)"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:s,props:{alt:B,src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002F11554_2017_670_Fig7_HTML.gif"},children:[]}]},{type:a,value:c},{type:b,tag:E,props:{},children:[]},{type:a,value:c},{type:b,tag:F,props:{id:"mrsmap-vo"},children:[{type:b,tag:f,props:{href:"#mrsmap-vo",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:"MRSMAP VO"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Stückler et al. ["},{type:b,tag:f,props:{href:"https:\u002F\u002Flink.springer.com\u002Farticle\u002F10.1007\u002Fs11554-017-0670-y#CR56",rel:[m,n,o],target:p},children:[{type:a,value:"56"}]},{type:a,value:"] proposed a 3D feature-based approach with a frame-to-frame matching strategy in which each frame is viewed as an octree of surfels. The originality of the approach is that multiple levels of resolution can be used simultaneously since each parent node of the octree encodes the information of their children node"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:s,props:{alt:B,src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002F11554_2017_670_Fig8_HTML.gif"},children:[]}]},{type:a,value:c},{type:b,tag:E,props:{},children:[]},{type:a,value:c},{type:b,tag:F,props:{id:"occipital-sttracker"},children:[{type:b,tag:f,props:{href:"#occipital-sttracker",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:"Occipital STTracker"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Structure is a depth sensor manufactured by Occipital using Primesense’s technology, and it uses structured light to estimate the depth. The sensor does not support any RGB camera, and it has to take advantage of the mobile device rear camera to retrieve the RGB frames. Occipital provides an iOS SDK with a VO algorithm in two flavours: depth-based ["},{type:b,tag:f,props:{href:"https:\u002F\u002Flink.springer.com\u002Farticle\u002F10.1007\u002Fs11554-017-0670-y#CR35",rel:[m,n,o],target:p},children:[{type:a,value:"35"}]},{type:a,value:"] and hybrid ["},{type:b,tag:f,props:{href:"https:\u002F\u002Flink.springer.com\u002Farticle\u002F10.1007\u002Fs11554-017-0670-y#CR36",rel:[m,n,o],target:p},children:[{type:a,value:"36"}]},{type:a,value:"]."}]},{type:a,value:c},{type:b,tag:E,props:{},children:[]},{type:a,value:c},{type:b,tag:F,props:{id:"fasticp"},children:[{type:b,tag:f,props:{href:"#fasticp",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:"FastICP"}]},{type:a,value:c},{type:b,tag:t,props:{id:ad},children:[{type:b,tag:f,props:{href:"#rangeflow",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:ae}]},{type:a,value:c},{type:b,tag:t,props:{id:af},children:[{type:b,tag:f,props:{href:"#3d-ndt",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:ag}]},{type:a,value:c},{type:b,tag:t,props:{id:ah},children:[{type:b,tag:f,props:{href:"#ccny",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:ai}]},{type:a,value:c},{type:b,tag:t,props:{id:aj},children:[{type:b,tag:f,props:{href:"#demo",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:ak}]},{type:a,value:c},{type:b,tag:E,props:{},children:[]},{type:a,value:c},{type:b,tag:D,props:{id:al},children:[{type:b,tag:f,props:{href:"#algorithm-comparisions",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:am}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Reference :  "},{type:b,tag:f,props:{href:aO,rel:[m,n,o],target:p},children:[{type:a,value:"the reference article"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Dataset used : "},{type:b,tag:f,props:{href:aP,rel:[m,n,o],target:p},children:[{type:a,value:"RGB-D TUM"}]}]},{type:a,value:c},{type:b,tag:t,props:{id:an},children:[{type:b,tag:f,props:{href:"#accuracy",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:ao}]},{type:a,value:c},{type:b,tag:F,props:{id:"evaluation"},children:[{type:b,tag:f,props:{href:"#evaluation",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:"Evaluation"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"There are two well-known metrics that can be used to estimate the accuracy of the estimated camera poses over time, the absolute translational error (ATE) and the translational relative pose error (RPE)."}]},{type:a,value:c},{type:b,tag:A,props:{},children:[{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"ATE computes the Euclidean distance between the estimated camera position and its ground truth"}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"The ATE is then defined as the mean squared error (RMSE) of these distances all along the trajectory."}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"The RPE is instead used to measure the local accuracy of the estimated trajectory over a fixed time interval Δ"}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"TUM has various environment dataset : The “fr1” sequences provide various scenes recorded in an office environment."}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"The “fr2” sequences were recorded in a large industrial hall. Compared to the “fr1” sequences they are generally longer and have a slower camera motion."}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"the “fr3” sequences feature a scene with a desk and various evaluation series to evaluate the performances of algorithms on scenes with structure and\u002For texture."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:t,props:{id:ap},children:[{type:b,tag:f,props:{href:"#results",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:aq}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:s,props:{alt:B,src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002FSelection_069.png"},children:[]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:q,props:{},children:[{type:a,value:"Lower RPE Better accuracy."}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:s,props:{alt:B,src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002FSelection_065.png"},children:[]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:s,props:{alt:B,src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002FSelection_066.png"},children:[]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:s,props:{alt:B,src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002FSelection_068.png"},children:[]}]},{type:a,value:c},{type:b,tag:t,props:{id:ar},children:[{type:b,tag:f,props:{href:"#results-outline",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:as}]},{type:a,value:c},{type:b,tag:A,props:{},children:[{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"the hybrid and image-based methods are the most accurate when the environment has texture and no structure such as the scenes “fr1 floor”, “fr3 nostructure texture near withloop” and “fr3 nostructure texture far”"}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"Similarly, the environments with structure and low texture favour the hybrid and depth-based algorithms as shown by the scene “fr3 structure notexture near”"}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"noisier depth data, the accuracy of the ICP algorithm is comparable to the image-based algorithms"}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"When the environment is neither flat nor textureless, image-based or hybrid-based methods are more robust than depth-based methods"}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"the scenes recorded in the office also show the hybrid and image-based methods are more robust, but the accuracy difference with the depth-based methods is slighter"}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"OCV RGBD ICP [Hybrid] and Fovis "},{type:b,tag:u,props:{},children:[{type:a,value:"[Feature Based]"}]},{type:a,value:" have the lowest RPE (Most accurate) on the scenes “fr1” and “fr2”"}]},{type:a,value:c},{type:b,tag:l,props:{},children:[{type:a,value:"DVO [Depth-based] and OCV RGB-D [Image-based direct] generally come behind or between"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:E,props:{},children:[]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"A first simple observation of the different graphs is that the accuracy results significantly vary from a scene to another. As stated by Fang ["},{type:b,tag:f,props:{href:"https:\u002F\u002Flink.springer.com\u002Farticle\u002F10.1007\u002Fs11554-017-0670-y#CR13",rel:[m,n,o],target:p},children:[{type:a,value:"13"}]},{type:a,value:"], there is no algorithm which outperforms the others in all environments. The results have to be analysed w.r.t. the scene characteristics. Therefore, the choice of VO algorithm depends on the target environment."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Apart from the challenging scenes, we described earlier and correspond to higher RPE values, the slower “fr2” (dataset in a certain environment) scenes obtain better results than the “fr1” scenes."}]},{type:a,value:c},{type:b,tag:t,props:{id:at},children:[{type:b,tag:f,props:{href:"#other-comparisions",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:au}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:f,props:{href:"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpmc\u002Farticles\u002FPMC4063070\u002F",rel:[m,n,o],target:p},children:[{type:a,value:"Morell-Gimenez et al"}]},{type:a,value:". performed a comparision of registation methods on scenes mapping and object reconstruction scenarios. for the scenes mapping scenario which is our topic of interest, they evaluated five different algorithms : DVO, KinFu (Kinect Fusion implementation), an ICP approach, an image-based visual feature approach using a combiation of FAST keypoints and BRIEF descriptors, and an hybrid two-stage approach combining the two last ones where the refinement step is provided by the ICP algorithm. The approaches implemented by Morell-Gimnenez et al. using Point Cloud Library. "},{type:b,tag:q,props:{},children:[{type:a,value:"The results show that DVO and KinFu are the most accurate algorithms on the \"fr1\" scenes of the TUM datasets"}]},{type:a,value:". However there is no information about the computational time and the memory consumption as the main objective was to assess the quality and the accuracy of each method."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:f,props:{href:aQ,rel:[m,n,o],target:p},children:[{type:a,value:"ICL-NUIM"}]},{type:a,value:" provided a dataset as well as evaluation of 5 algorithms (DVO, Fovis, RGB-D, ICP KinectFustion flavour, and Kintinuous) in all scenes from ICL-NUIM Dataset with the ATE metric "},{type:b,tag:q,props:{},children:[{type:a,value:"showed a clear advantages to KinectFusion ICP registration while Fovis gives the less accurate results."}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:f,props:{href:"https:\u002F\u002Fjournals.sagepub.com\u002Fdoi\u002Fpdf\u002F10.5772\u002F59991",rel:[m,n,o],target:p},children:[{type:a,value:"Fang and Zhang"}]},{type:a,value:" compared different open-source VO : Libviso2, Fovis, DVO, Fast ICP, Rangeflow, 3D-NDT, CCNY, DEMO on TIM Dataset and also a challenging dataset created by the authors with illumination changes , fast motions and long corridors. "},{type:b,tag:q,props:{},children:[{type:a,value:"Their study shows that there is no algorithm performing well in all environments and they provided some guidelines to choose a VO algorithm depending on the environment. For example, when the scene is well illuminated, image-based and hybrid methods are recommended, whereas depth-based methods are only really interesting in low light environments."}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:f,props:{href:aO,rel:[m,n,o],target:p},children:[{type:a,value:"Vincent Angladon et al. "}]},{type:a,value:" They are proposing comparisons in their paper similar to above comparisons while they are focusing on mobile device experiments which has some performance and memory limitations. They are using the algorithms similar to Fang and Zhang and also some other algorithms that were not included in that. For the algorithm selection, they have considered the methods that performed better in other benchmark studies and their code were available. They have selected DVO, Fovis, MRSMAP, Three algorithms of OpenCV RGB-D Module OCV-ICP, OCV-RGB-D and OCV-RgbdICP)  and VO Algorithms that come with "},{type:b,tag:f,props:{href:"https:\u002F\u002Fstructure.io\u002F",rel:[m,n,o],target:p},children:[{type:a,value:"Occipital sensor"}]},{type:a,value:aR}]},{type:a,value:c},{type:b,tag:t,props:{id:av},children:[{type:b,tag:f,props:{href:"#semi-dense-monocular-vo-algorithms-schops-et-al",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:"Semi-dense Monocular VO algorithms "},{type:b,tag:u,props:{},children:[{type:a,value:"Schops et al."}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"They achieved a 30 FPS tracking performance with a partial porting of the semi-dens LSD-Slam algorithm on a Sony Experia Z1 phone.However there is not code available publicly"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:f,props:{href:aS,rel:[m,n,o],target:p},children:[{type:b,tag:s,props:{alt:aT,src:"https:\u002F\u002Fimg.youtube.com\u002Fvi\u002FX0hx2vxxTMg\u002Fmaxresdefault.jpg"},children:[]}]}]},{type:a,value:c},{type:b,tag:t,props:{id:aw},children:[{type:b,tag:f,props:{href:"#rgb-d-vslam-algorithm-based-on-slamdunk",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:ax}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"It can run on a Samsung Galaxy Tab Pro 10.1 tablet . but due to the lack of low level optimizations it could only reach 10 FPS."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:f,props:{href:"https:\u002F\u002Fgithub.com\u002Fm4nh\u002Fskimap_ros\u002Fwiki\u002FSlamDunk-Tracker-with-SkiMap",rel:[m,n,o],target:p},children:[{type:a,value:"There is package for ROS implementing SlamDunk"}]},{type:a,value:aR}]},{type:a,value:c},{type:b,tag:D,props:{id:ay},children:[{type:b,tag:f,props:{href:"#datasets",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:az}]},{type:a,value:c},{type:b,tag:t,props:{id:aA},children:[{type:b,tag:f,props:{href:"#tum-dataset",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:b,tag:f,props:{href:aP,rel:[m,n,o],target:p},children:[{type:a,value:aB}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"a collection of different RGB-D image sequences meant to benchmark the SLAM and visual odometry algorithms. The data was recorded at full frame rate (30 Hz) and sensor resolution (640x480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, they provide the "},{type:b,tag:q,props:{},children:[{type:a,value:"accelerometer"}]},{type:a,value:" data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems."}]},{type:a,value:c},{type:b,tag:t,props:{id:aC},children:[{type:b,tag:f,props:{href:"#kitti-dataset",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:b,tag:f,props:{href:"http:\u002F\u002Fwww.cvlibs.net\u002Fdatasets\u002Fkitti\u002F",rel:[m,n,o],target:p},children:[{type:a,value:aD}]}]},{type:a,value:c},{type:b,tag:t,props:{id:aE},children:[{type:b,tag:f,props:{href:"#icl-nuim-dataset",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:b,tag:f,props:{href:aQ,rel:[m,n,o],target:p},children:[{type:a,value:aF}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Handa created the ICL-NUIM dataset composed of synthetic images of indoor scenes generated by POVRay. Although the main foucs of the dataset is to provide a method to benchmark the surface reconstruction accuracy, it has been used to evaluate differenet VO algorithms, thanks to the ground truth provided by the synthetic data."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:f,props:{href:aS,rel:[m,n,o],target:p},children:[{type:b,tag:s,props:{alt:aT,src:"https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002F1555007376821.png"},children:[]}]}]},{type:a,value:c},{type:b,tag:D,props:{id:aG},children:[{type:b,tag:f,props:{href:"#important-notes-from-the-article",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:aH}]},{type:a,value:c},{type:b,tag:t,props:{id:aI},children:[{type:b,tag:f,props:{href:"#small-size-and-embedded-depth-sensors",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:aJ}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:q,props:{},children:[{type:a,value:"PrimeSense"}]},{type:a,value:" proposed first the now discontinued Capri 1.25 embedded camera sensor, which later the company bought by Apple for 365$ Million.\n"},{type:b,tag:f,props:{href:"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=5qsgmKgMQnM",rel:[m,n,o],target:p},children:[{type:b,tag:q,props:{},children:[{type:a,value:"Google Tango Penaut"}]}]},{type:a,value:" and "},{type:b,tag:q,props:{},children:[{type:a,value:"Yellow stone"}]},{type:a,value:" are  another projectswere  aiming on using depth sensors on mobile devices and embedded devices.\n"},{type:b,tag:q,props:{},children:[{type:a,value:"Intel realsense smartphone"}]},{type:a,value:" is also a smartphone using intel depth sensors.\n"},{type:b,tag:s,props:{alt:"enter image description here",src:"https:\u002F\u002Fstatic.techspot.com\u002Fimages2\u002Fnews\u002Fbigimage\u002F2016\u002F01\u002F2016-01-07-image-12.jpg"},children:[]}]},{type:a,value:c},{type:b,tag:D,props:{id:aK},children:[{type:b,tag:f,props:{href:"#questions",ariaHidden:h,tabIndex:i},children:[{type:b,tag:d,props:{className:[j,k]},children:[]}]},{type:a,value:aL}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"What is ATE Metric ?"}]}]},dir:"\u002Farticles",path:"\u002Farticles\u002Fvisualodometry",extension:".md",updatedAt:"2021-01-02T18:13:33.516Z"}}],fetch:[],mutations:void 0}}("text","element","\n","span","p","a","token","true",-1,"icon","icon-link","li","nofollow","noopener","noreferrer","_blank","strong",3,"img","h3","em","code","inserted","prefix","line","deleted","ul","",2,"h2","hr","h4","blockquote","div","nuxt-content-highlight","pre","language-diff","line-numbers","inserted-sign","+"," Advantages:\n","deleted-sign","-"," Disadvantages\n","h5","https:\u002F\u002Fgithub.com\u002FHamedJafarzadeh\u002FVisualOdometry\u002Fraw\u002Fmaster\u002Fimgs\u002F1554888824498.png",true,"abstract","Abstract","introduction","Introduction","keynotes-from-the-reference-articlehttpslinkspringercomarticle101007s11554-017-0670-y","Keynotes from the [reference article][https:\u002F\u002Flink.springer.com\u002Farticle\u002F10.1007\u002Fs11554-017-0670-y]","rgb-d-visual-odometry-methods","RGB-d Visual odometry methods","algortihms","Algortihms","rangeflow","Rangeflow","3d-ndt","3D-NDT","ccny","CCNY","demo","Demo","algorithm-comparisions","Algorithm comparisions","accuracy","Accuracy","results","Results","results-outline","Results outline","other-comparisions","Other comparisions","semi-dense-monocular-vo-algorithms-schops-et-al","rgb-d-vslam-algorithm-based-on-slamdunk","RGB-D vSLAM algorithm based on SlamDunk","datasets","Datasets","tum-dataset","TUM Dataset","kitti-dataset","KITTI Dataset","icl-nuim-dataset"," ICL-NUIM Dataset","important-notes-from-the-article","Important notes from the article","small-size-and-embedded-depth-sensors","Small size and embedded depth sensors","questions","Questions","Depth-based","| Image-Based | Direct","https:\u002F\u002Flink.springer.com\u002Farticle\u002F10.1007\u002Fs11554-017-0670-y","https:\u002F\u002Fvision.in.tum.de\u002Fdata\u002Fdatasets\u002Frgbd-dataset","https:\u002F\u002Fwww.doc.ic.ac.uk\u002F~ahanda\u002FVaFRIC\u002Ficlnuim.html",".","https:\u002F\u002Fyoutu.be\u002FX0hx2vxxTMg","Watch the video")));